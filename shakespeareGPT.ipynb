{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ShakespeareGPT\n",
    "\n",
    "This notebook is a very basic model to generate Shakespearean text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "text_data = urllib.request.urlopen('https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt').read().decode('utf-8')\n",
    "print(len(text_data))\n",
    "print(text_data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text_data)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be building a character level model to generate text. So, here for each character we create encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda x: [stoi[ch] for ch in x]\n",
    "decode = lambda x: ''.join([itos[i] for i in x])\n",
    "\n",
    "print(encode('hello'))\n",
    "print(decode(encode('hello')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394])\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(data = encode(text_data), dtype = torch.long)\n",
    "print(data.shape)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(data) * 0.9)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 8\n",
    "BATCH_SIZE = 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n"
     ]
    }
   ],
   "source": [
    "print(train_data[:BLOCK_SIZE + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: tensor([18]) Target: 47\n",
      "Context: tensor([18, 47]) Target: 56\n",
      "Context: tensor([18, 47, 56]) Target: 57\n",
      "Context: tensor([18, 47, 56, 57]) Target: 58\n",
      "Context: tensor([18, 47, 56, 57, 58]) Target: 1\n",
      "Context: tensor([18, 47, 56, 57, 58,  1]) Target: 15\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15]) Target: 47\n",
      "Context: tensor([18, 47, 56, 57, 58,  1, 15, 47]) Target: 58\n"
     ]
    }
   ],
   "source": [
    "# We want the transformer to predict the next character given the previous characters from the range of 1 to BLOCK_SIZE\n",
    "n_embed = 32\n",
    "for i in range(BLOCK_SIZE):\n",
    "    context = train_data[:i + 1]\n",
    "    target = train_data[i + 1]\n",
    "    print(f\"Context: {context} Target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data: torch.Tensor, \n",
    "              batch_size: int, \n",
    "              block_size: int):\n",
    "    index = torch.randint(len(data) - block_size, (batch_size,)) # Randomly selects an index for each batch\n",
    "    x = torch.stack([data[i:i + block_size] for i in index])\n",
    "    y = torch.stack([data[i + 1:i + 1 + block_size] for i in index])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n",
      "tensor([[46, 47, 57,  1, 56, 43, 39, 57],\n",
      "        [53, 52, 43,  6,  1, 57, 53,  1],\n",
      "        [41, 53, 51, 43,  1, 61, 46, 43],\n",
      "        [50, 39, 57, 54,  5, 42,  1, 51]])\n",
      "torch.Size([4, 8])\n",
      "tensor([[47, 57,  1, 56, 43, 39, 57, 53],\n",
      "        [52, 43,  6,  1, 57, 53,  1, 51],\n",
      "        [53, 51, 43,  1, 61, 46, 43, 52],\n",
      "        [39, 57, 54,  5, 42,  1, 51, 63]])\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch(train_data, BATCH_SIZE, BLOCK_SIZE)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Language Model\n",
    "\n",
    "We start simple with a bigram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(num_embeddings = vocab_size, embedding_dim = n_embed)\n",
    "        self.positional_embedding = nn.Embedding(num_embeddings = BLOCK_SIZE, embedding_dim = n_embed)\n",
    "        self.linear = nn.Linear(in_features = n_embed, out_features = vocab_size)\n",
    "        # Each row corresponds to a token in the vocabulary. The dimensionality of the embedding is vocab_size here\n",
    "    def forward(self, x, target):\n",
    "        B,T = x.shape\n",
    "        token_embedding = self.token_embedding(x)\n",
    "        pos_embedding = self.positional_embedding(torch.arange(T, device = device))\n",
    "        token_embedding += pos_embedding\n",
    "        logits = self.linear(token_embedding)\n",
    "        if target is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            target = target.view(B*T)\n",
    "            # If the model is trained well, logits for a given input will be high for the correct target\n",
    "            loss = nn.functional.cross_entropy(logits,target)\n",
    "        return logits, loss\n",
    "    def generate(self, x, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits,_ = self.forward(x,None) # Select for all the batches, the last token\n",
    "            logits = logits[:, -1, :] # In all batches, pick the last token\n",
    "            probs = nn.functional.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            x = torch.cat([x,next_token],dim=1)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBigramLanguageModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\warre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\warre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\warre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\warre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(decode(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()))\n",
      "Cell \u001b[1;32mIn[13], line 28\u001b[0m, in \u001b[0;36mBigramLanguageModel.generate\u001b[1;34m(self, x, max_new_tokens)\u001b[0m\n\u001b[0;32m     26\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;66;03m# In all batches, pick the last token\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m     next_token \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x,next_token],dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros((1,1), dtype = torch.long).to(device)\n",
    "print(decode(model.generate(x, max_new_tokens = 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000 Loss 2.8726345167160034\n",
      "Epoch 2000 Loss 2.5273202818498612\n",
      "Epoch 3000 Loss 2.4952911411684617\n",
      "Epoch 4000 Loss 2.4791210903202456\n",
      "Epoch 5000 Loss 2.480335673691181\n",
      "Epoch 6000 Loss 2.4748337511628877\n",
      "Epoch 7000 Loss 2.4704458322767824\n",
      "Epoch 8000 Loss 2.4693622655477765\n",
      "Epoch 9000 Loss 2.4754364204473105\n",
      "Epoch 10000 Loss 2.4661280247879094\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "model.train()\n",
    "average_loss = 0\n",
    "for epoch in range(10000):\n",
    "    x,y = get_batch(train_data, BATCH_SIZE, BLOCK_SIZE)\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    logits, loss = model(x,y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    average_loss += loss.item()\n",
    "    if (epoch + 1)% 1000 == 0:\n",
    "        average_loss /= 1000\n",
    "        print(f\"Epoch {epoch + 1} Loss {average_loss}\")\n",
    "    optimizer.step()\n",
    "    # print(f\"Epoch {epoch} Loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but I wou, os an werd's Go,\n",
      "Marnutowour fres fit ms alllllin:\n",
      "carefftharar mate INCHAisolendy'seachonice hedegon m; r hy l tr:\n",
      "WICHAMy s, uthoveg:\n",
      "tceryour tho mofene bangertoyTo the is, CLI Tu ck nd, hthevown:\n",
      "He,\n",
      "Toronun me Jur?\n",
      "Wher youn cee e, IIVIS:\n",
      "Thadre p os thacorofu bupr,\n",
      "\n",
      "\n",
      "Fore arpo-f GAs me,\n",
      "\n",
      "Weteeairamavangout d aigm ICotwed Mulend d nd bu th'ss simy thiskederond\n",
      "\n",
      "Tandl hisonothomeliny ad, s,\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(x, max_new_tokens = 400)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including Self Attention\n",
    "\n",
    "A given token would have to communicate with previous tokens. A token cannot see the future tokens. So, we will be using self attention to include the context of previous tokens. We can implement this using a weighted sum for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 32])\n"
     ]
    }
   ],
   "source": [
    "B,T,C = 4,8,32 # Batch size, sequence length, channels\n",
    "x = torch.randn(B,T,C)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 5.3561e-01,  2.9674e+00, -8.7036e-01,  ...,  3.1456e-01,\n",
      "           8.7163e-02,  1.7175e-01],\n",
      "         [ 9.4755e-01,  1.1643e+00,  1.0084e-01,  ..., -1.2896e+00,\n",
      "          -5.6144e-01, -3.4468e-02],\n",
      "         [ 2.6413e-01,  5.3581e-01, -3.3713e-01,  ..., -9.9964e-01,\n",
      "          -2.2322e-01, -5.6221e-01],\n",
      "         ...,\n",
      "         [ 1.1283e-01,  5.2472e-02,  1.3156e-01,  ..., -1.7826e-01,\n",
      "           2.8710e-01, -8.2842e-01],\n",
      "         [ 2.5710e-02, -1.1048e-01,  1.7121e-01,  ..., -1.5548e-01,\n",
      "           2.8859e-01, -7.2835e-01],\n",
      "         [ 1.1724e-01, -2.3913e-01,  3.0612e-01,  ..., -3.0154e-01,\n",
      "           4.7531e-01, -7.1948e-01]],\n",
      "\n",
      "        [[-3.6567e-01,  1.0631e+00, -1.2801e+00,  ...,  1.3109e-01,\n",
      "           1.2726e+00, -1.1563e+00],\n",
      "         [-7.8052e-01,  5.7265e-01, -2.0448e-01,  ..., -3.8020e-01,\n",
      "           4.8519e-01, -1.6571e-01],\n",
      "         [-3.9725e-01,  4.9517e-01, -4.3468e-01,  ..., -2.1668e-03,\n",
      "           1.2573e-01, -2.1303e-02],\n",
      "         ...,\n",
      "         [-5.3013e-02,  7.5932e-02, -1.4797e-01,  ..., -4.6662e-01,\n",
      "          -1.6648e-01, -1.2336e-01],\n",
      "         [-5.6101e-02,  1.3949e-01, -7.4790e-02,  ..., -9.3806e-02,\n",
      "          -1.1766e-01, -1.5945e-01],\n",
      "         [-8.4113e-02,  2.7895e-01, -1.7400e-01,  ..., -2.0317e-01,\n",
      "          -8.0100e-02,  2.3264e-03]],\n",
      "\n",
      "        [[-5.2913e-01, -9.5701e-01, -7.8699e-02,  ..., -6.5722e-01,\n",
      "          -8.3353e-01, -1.0094e+00],\n",
      "         [-5.5407e-01, -1.3534e-01, -5.8927e-01,  ...,  1.4633e-01,\n",
      "          -8.4694e-01, -5.0510e-01],\n",
      "         [ 1.3103e-01,  3.8229e-01, -2.2350e-01,  ...,  8.7081e-01,\n",
      "          -7.2420e-01, -4.8626e-01],\n",
      "         ...,\n",
      "         [ 1.2647e-01,  5.1689e-01, -3.0651e-01,  ...,  4.1650e-01,\n",
      "          -3.3024e-01,  2.2460e-01],\n",
      "         [ 1.7022e-01,  3.6963e-01, -2.3906e-01,  ...,  3.6325e-01,\n",
      "          -2.4417e-01,  3.2196e-01],\n",
      "         [ 8.3894e-02,  2.2104e-01, -1.9583e-02,  ...,  4.6612e-01,\n",
      "          -1.2731e-01,  3.5118e-01]],\n",
      "\n",
      "        [[ 3.3694e-01,  1.4870e+00, -3.4668e-01,  ..., -2.1457e-01,\n",
      "           8.2886e-01, -1.9947e-01],\n",
      "         [ 9.9025e-01, -9.2286e-02,  2.1020e-01,  ..., -4.9491e-01,\n",
      "           7.5866e-01, -2.6195e-01],\n",
      "         [ 9.3349e-01, -8.1701e-01,  5.9674e-03,  ..., -6.0049e-01,\n",
      "           3.1338e-01, -4.5077e-01],\n",
      "         ...,\n",
      "         [ 9.5251e-02,  1.0126e-02,  1.6199e-01,  ..., -3.3747e-01,\n",
      "           1.3847e-01, -1.6047e-01],\n",
      "         [-6.9557e-02, -2.6073e-01, -4.9864e-03,  ..., -1.7431e-01,\n",
      "          -2.1710e-02, -1.4761e-01],\n",
      "         [-3.6348e-01, -1.2173e-01,  1.6691e-02,  ..., -2.5242e-01,\n",
      "          -7.0955e-03, -2.9922e-01]]])\n",
      "tensor([[[ 5.3561e-01,  2.9674e+00, -8.7036e-01,  ...,  3.1456e-01,\n",
      "           8.7163e-02,  1.7175e-01],\n",
      "         [ 1.3595e+00, -6.3882e-01,  1.0720e+00,  ..., -2.8937e+00,\n",
      "          -1.2100e+00, -2.4068e-01],\n",
      "         [-1.1027e+00, -7.2120e-01, -1.2131e+00,  ..., -4.1982e-01,\n",
      "           4.5322e-01, -1.6177e+00],\n",
      "         ...,\n",
      "         [-8.0408e-01, -1.3657e+00, -3.0080e-01,  ...,  6.7539e-01,\n",
      "           3.6956e-01, -5.4293e-01],\n",
      "         [-4.9700e-01, -1.0882e+00,  4.0917e-01,  ..., -1.8828e-02,\n",
      "           2.9755e-01, -1.2796e-01],\n",
      "         [ 7.5791e-01, -1.1397e+00,  1.2504e+00,  ..., -1.3239e+00,\n",
      "           1.7824e+00, -6.5739e-01]],\n",
      "\n",
      "        [[-3.6567e-01,  1.0631e+00, -1.2801e+00,  ...,  1.3109e-01,\n",
      "           1.2726e+00, -1.1563e+00],\n",
      "         [-1.1954e+00,  8.2191e-02,  8.7117e-01,  ..., -8.9149e-01,\n",
      "          -3.0223e-01,  8.2492e-01],\n",
      "         [ 3.6930e-01,  3.4021e-01, -8.9507e-01,  ...,  7.5390e-01,\n",
      "          -5.9318e-01,  2.6752e-01],\n",
      "         ...,\n",
      "         [ 1.2356e+00, -3.7836e-01,  7.4508e-01,  ...,  5.3932e-02,\n",
      "          -1.3653e+00,  8.5221e-02],\n",
      "         [-7.4628e-02,  5.2085e-01,  3.6431e-01,  ...,  2.1431e+00,\n",
      "           1.7528e-01, -3.7596e-01],\n",
      "         [-2.8020e-01,  1.2551e+00, -8.6844e-01,  ..., -9.6875e-01,\n",
      "           1.8281e-01,  1.1347e+00]],\n",
      "\n",
      "        [[-5.2913e-01, -9.5701e-01, -7.8699e-02,  ..., -6.5722e-01,\n",
      "          -8.3353e-01, -1.0094e+00],\n",
      "         [-5.7900e-01,  6.8634e-01, -1.0998e+00,  ...,  9.4989e-01,\n",
      "          -8.6034e-01, -8.2968e-04],\n",
      "         [ 1.5012e+00,  1.4175e+00,  5.0804e-01,  ...,  2.3198e+00,\n",
      "          -4.7873e-01, -4.4857e-01],\n",
      "         ...,\n",
      "         [ 3.3868e-01,  1.2491e+00, -4.3429e-01,  ..., -1.7460e-01,\n",
      "          -9.0558e-01,  3.1115e-01],\n",
      "         [ 4.3272e-01, -5.1393e-01,  1.6561e-01,  ...,  4.3748e-02,\n",
      "           2.7222e-01,  9.0612e-01],\n",
      "         [-5.2038e-01, -8.1909e-01,  1.5168e+00,  ...,  1.1862e+00,\n",
      "           6.9073e-01,  5.5577e-01]],\n",
      "\n",
      "        [[ 3.3694e-01,  1.4870e+00, -3.4668e-01,  ..., -2.1457e-01,\n",
      "           8.2886e-01, -1.9947e-01],\n",
      "         [ 1.6435e+00, -1.6716e+00,  7.6708e-01,  ..., -7.7524e-01,\n",
      "           6.8845e-01, -3.2443e-01],\n",
      "         [ 8.1997e-01, -2.2665e+00, -4.0250e-01,  ..., -8.1164e-01,\n",
      "          -5.7718e-01, -8.2842e-01],\n",
      "         ...,\n",
      "         [ 1.1344e+00,  1.9220e+00,  7.0726e-01,  ...,  7.4015e-03,\n",
      "          -2.3554e+00,  1.0897e+00],\n",
      "         [-1.0584e+00, -1.8859e+00, -1.0068e+00,  ...,  8.0464e-01,\n",
      "          -9.8282e-01, -7.0432e-02],\n",
      "         [-2.4210e+00,  8.5131e-01,  1.6844e-01,  ..., -7.9918e-01,\n",
      "           9.5205e-02, -1.3605e+00]]])\n"
     ]
    }
   ],
   "source": [
    "xbow = torch.zeros((B,T,C)) #bow = bag of words\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,: t+1] # (t,c)\n",
    "        xbow[b,t] = torch.mean(xprev,dim=0)\n",
    "\n",
    "print(xbow)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the above more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "tensor([[4., 4., 1.],\n",
      "        [7., 6., 8.],\n",
      "        [7., 1., 1.]])\n",
      "tensor([[4.0000, 4.0000, 1.0000],\n",
      "        [5.5000, 5.0000, 4.5000],\n",
      "        [6.0000, 3.6667, 3.3333]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / torch.sum(a, dim = 1, keepdim=True)\n",
    "print(a)\n",
    "b = torch.randint(0, 10, (3, 3)).float()\n",
    "print(b)\n",
    "print(torch.matmul(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "averaging_matrix = torch.tril(torch.ones(T, T))\n",
    "averaging_matrix /= torch.sum(averaging_matrix, dim = 1, keepdim = True)\n",
    "print(averaging_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 5.3561e-01,  2.9674e+00, -8.7036e-01,  ...,  3.1456e-01,\n",
      "           8.7163e-02,  1.7175e-01],\n",
      "         [ 9.4755e-01,  1.1643e+00,  1.0084e-01,  ..., -1.2896e+00,\n",
      "          -5.6144e-01, -3.4468e-02],\n",
      "         [ 2.6413e-01,  5.3581e-01, -3.3713e-01,  ..., -9.9964e-01,\n",
      "          -2.2322e-01, -5.6221e-01],\n",
      "         ...,\n",
      "         [ 1.1283e-01,  5.2472e-02,  1.3156e-01,  ..., -1.7826e-01,\n",
      "           2.8710e-01, -8.2842e-01],\n",
      "         [ 2.5710e-02, -1.1048e-01,  1.7121e-01,  ..., -1.5548e-01,\n",
      "           2.8859e-01, -7.2835e-01],\n",
      "         [ 1.1724e-01, -2.3913e-01,  3.0612e-01,  ..., -3.0154e-01,\n",
      "           4.7531e-01, -7.1948e-01]],\n",
      "\n",
      "        [[-3.6567e-01,  1.0631e+00, -1.2801e+00,  ...,  1.3109e-01,\n",
      "           1.2726e+00, -1.1563e+00],\n",
      "         [-7.8052e-01,  5.7265e-01, -2.0448e-01,  ..., -3.8020e-01,\n",
      "           4.8519e-01, -1.6571e-01],\n",
      "         [-3.9725e-01,  4.9517e-01, -4.3468e-01,  ..., -2.1668e-03,\n",
      "           1.2573e-01, -2.1303e-02],\n",
      "         ...,\n",
      "         [-5.3013e-02,  7.5932e-02, -1.4797e-01,  ..., -4.6662e-01,\n",
      "          -1.6648e-01, -1.2336e-01],\n",
      "         [-5.6101e-02,  1.3949e-01, -7.4790e-02,  ..., -9.3806e-02,\n",
      "          -1.1766e-01, -1.5945e-01],\n",
      "         [-8.4113e-02,  2.7895e-01, -1.7400e-01,  ..., -2.0317e-01,\n",
      "          -8.0100e-02,  2.3264e-03]],\n",
      "\n",
      "        [[-5.2913e-01, -9.5701e-01, -7.8699e-02,  ..., -6.5722e-01,\n",
      "          -8.3353e-01, -1.0094e+00],\n",
      "         [-5.5407e-01, -1.3534e-01, -5.8927e-01,  ...,  1.4633e-01,\n",
      "          -8.4694e-01, -5.0510e-01],\n",
      "         [ 1.3103e-01,  3.8229e-01, -2.2350e-01,  ...,  8.7081e-01,\n",
      "          -7.2420e-01, -4.8626e-01],\n",
      "         ...,\n",
      "         [ 1.2647e-01,  5.1689e-01, -3.0651e-01,  ...,  4.1650e-01,\n",
      "          -3.3024e-01,  2.2460e-01],\n",
      "         [ 1.7022e-01,  3.6963e-01, -2.3906e-01,  ...,  3.6325e-01,\n",
      "          -2.4417e-01,  3.2196e-01],\n",
      "         [ 8.3894e-02,  2.2104e-01, -1.9583e-02,  ...,  4.6612e-01,\n",
      "          -1.2731e-01,  3.5118e-01]],\n",
      "\n",
      "        [[ 3.3694e-01,  1.4870e+00, -3.4668e-01,  ..., -2.1457e-01,\n",
      "           8.2886e-01, -1.9947e-01],\n",
      "         [ 9.9025e-01, -9.2286e-02,  2.1020e-01,  ..., -4.9491e-01,\n",
      "           7.5866e-01, -2.6195e-01],\n",
      "         [ 9.3349e-01, -8.1701e-01,  5.9674e-03,  ..., -6.0049e-01,\n",
      "           3.1338e-01, -4.5077e-01],\n",
      "         ...,\n",
      "         [ 9.5251e-02,  1.0126e-02,  1.6199e-01,  ..., -3.3747e-01,\n",
      "           1.3847e-01, -1.6047e-01],\n",
      "         [-6.9557e-02, -2.6073e-01, -4.9865e-03,  ..., -1.7431e-01,\n",
      "          -2.1710e-02, -1.4761e-01],\n",
      "         [-3.6348e-01, -1.2173e-01,  1.6691e-02,  ..., -2.5242e-01,\n",
      "          -7.0955e-03, -2.9922e-01]]])\n"
     ]
    }
   ],
   "source": [
    "xbow2 = torch.matmul(averaging_matrix, x) # Since x has a batch dimension, pytorch will broadcast the averaging matrix to the batch dimension\n",
    "print(xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "# Another alternative method\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "weights = torch.zeros(T,T)\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "weights = torch.nn.functional.softmax(weights, dim = 1)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 16])\n",
      "torch.Size([4, 8, 16])\n",
      "torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(in_features = C, out_features = head_size, bias = False)\n",
    "query = nn.Linear(in_features = C, out_features = head_size, bias = False)\n",
    "value = nn.Linear(in_features = C, out_features = head_size, bias = False)\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "\n",
    "print(k.shape)\n",
    "print(q.shape)\n",
    "\n",
    "wei = torch.matmul(q, k.transpose(-2,-1)) # B,T,16 * B,16,T = B,T,T\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = torch.nn.functional.softmax(wei, dim = -1)\n",
    "v = value(x)\n",
    "\n",
    "out = torch.matmul(wei, v)\n",
    "print(out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
